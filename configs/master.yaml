# Master configuration â€” shared training defaults for all experiments.

training:
  epochs: 20
  batch_size: 16
  lr: 5.0e-5
  optimizer: adamw
  weight_decay: 0.01
  scheduler: cosine
  warmup_ratio: 0.05
  grad_clip: 1.0
  fp16: true

loss:
  lambda_kl: 1.0
  lambda_rcid: 1.0
  temperature: 2.0          # KD temperature for KL divergence

checkpoint_selection:
  top_k: 10
  diversity_ratio: 0.5

alignment:
  cka_strategy: greedy       # greedy | proportional
  procrustes: true

evaluation:
  perplexity_max_length: 512
  perplexity_stride: 256

seeds: [42, 123, 456]

logging:
  use_wandb: false
  log_every: 50              # steps between logging
  checkpoint_every: 5        # epochs between checkpoints

output_dir: outputs/results
