# Experiment 2 (Full): 7 Methods × 5 Tasks × 3 Seeds (Section 4.2)
#
# 完整方法对比实验：
#   7 蒸馏方法 × 5 回路任务 × 3 随机种子 = 105 组训练运行
#
# 与 exp2_distillation_comparison.yaml 的区别:
#   - 扩展到 7 种方法（增加 tinybert, minilm, informed_fitnets）
#   - 扩展到 5 个任务（增加 induction, sva, docstring）
#   - 增加 early stopping 和 OOD 评估

experiment:
  name: "exp2_full_comparison"
  output_dir: "outputs/results/exp2_full"
  seeds: [42, 123, 456]

# ── 模型架构 ─────────────────────────────────────────────────
teacher:
  name: "gpt2"
  n_layers: 12
  d_model: 768
  n_head: 12

student:
  # init: 学生初始化策略
  #   "distilgpt2" — 从预训练 DistilGPT-2 开始 fine-tune（推荐，已有语言能力）
  #   "scratch"    — 从零随机初始化（需大量训练数据）
  init: "distilgpt2"
  # 以下参数仅 init="scratch" 时生效；预训练模型自动读取架构
  n_layers: 6
  d_model: 768
  n_head: 12

# ── 任务 ─────────────────────────────────────────────────────
tasks:
  - name: "ioi"
    n_train_pairs: 400
    n_val_pairs: 100
  - name: "greater_than"
    n_train_pairs: 400
    n_val_pairs: 100
  - name: "induction"
    n_train_pairs: 400
    n_val_pairs: 100
  - name: "sva"
    n_train_pairs: 400
    n_val_pairs: 100
  - name: "docstring"
    n_train_pairs: 400
    n_val_pairs: 100

# ── 回路分析 ─────────────────────────────────────────────────
circuit:
  top_k_checkpoints: 5

# ── 跨模型对齐 ───────────────────────────────────────────────
alignment:
  calibration_size: 200

# ── 训练超参数（所有方法统一）──────────────────────────────────
training:
  batch_size: 32
  lr: 1.0e-5             # fine-tune 用较小 lr，避免破坏预训练权重
  max_epochs: 20          # fine-tune 不需要太多 epoch
  warmup_steps: 100
  max_grad_norm: 1.0
  eval_every_n_epochs: 5
  early_stopping_patience: 10
  early_stopping_metric: "task_accuracy"
  temperature: 4.0
  save_dir: "outputs/checkpoints/exp2_full"

  # RCID / InformedFitNets 的辅助损失权重
  lambda_rcid: 2.0        # 提高 RCID 权重，突出因果痕迹信号
  lambda_kl: 0.5          # 降低 KL 权重，避免淹没任务特定信号

# ── 蒸馏方法 ─────────────────────────────────────────────────
# method_name 值对应 trainer.py 中的 VALID_METHODS
methods:
  - method_name: "standard_kd"
    description: "Standard Knowledge Distillation (Hinton 2015)"

  - method_name: "fitnets"
    description: "FitNets intermediate feature matching (Romero 2015)"

  - method_name: "tinybert"
    description: "TinyBERT hidden+attention matching (Jiao 2020)"
    # TinyBERT 特有参数
    alpha: 1.0     # L_hidden 权重
    beta: 1.0      # L_attn 权重
    gamma: 1.0     # L_KL 权重

  - method_name: "minilm"
    description: "MiniLM Value relation matching (Wang 2020)"
    # MiniLM 特有参数
    alpha: 1.0     # L_VR 权重
    beta: 1.0      # L_KL 权重
    vr_temperature: 1.0

  - method_name: "prakash_cka"
    description: "Prakash CKA layer alignment (simplified)"

  - method_name: "informed_fitnets"
    description: "Ablation: RCID checkpoints + clean residual matching"
    # 使用与 RCID 完全相同的检查点和 W 矩阵
    use_rcid_checkpoints: true

  - method_name: "rcid"
    description: "RCID: Residual Causal Imprint Distillation (ours)"
    # 使用因果痕迹差值 d^T = h^T_clean - h^T_corrupt

# ── 评估指标 ─────────────────────────────────────────────────
eval:
  metrics:
    - "task_accuracy"
    - "causal_consistency"
    - "perplexity"
    - "ood_robustness"
  perplexity_dataset: "wikitext-2"
  perplexity_max_samples: 10000
  perplexity_stride: 512
